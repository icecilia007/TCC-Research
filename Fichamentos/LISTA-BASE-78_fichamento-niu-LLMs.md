# On Evaluating the Efficiency of Source Code Generated by LLMs

Niu, Changan; Zhang, Ting; Li, Chuanyi; Luo, Bin; Ng, Vincent. "On Evaluating the Efficiency of Source Code Generated by LLMs," AI Foundation Models and Software Engineering (FORGE ’24), April 14, 2024, Lisbon, Portugal, pp. 1–5. doi: [10.1145/3650105.3652295](https://doi.ieeecomputersociety.org/10.1145/3650105.3652295)

## 1. Fichamento de Conteúdo

O artigo aborda a questão frequentemente negligenciada da eficiência do código gerado por *Large Language Models* (LLMs), contrastando com avaliações comuns focadas apenas na correção funcional. Investiga dois pontos principais: a eficiência intrínseca do código de LLMs (RQ1) e como prompts específicos podem otimizar essa eficiência (RQ2). Para isso, empregou os benchmarks HumanEval, MBPP e um novo conjunto de problemas do LeetCode (LeetCodeEval), avaliando o tempo de execução normalizado, Pass@10 e porcentagem de *beats*. Os resultados da RQ1 revelaram que a capacidade de gerar código correto não se correlaciona diretamente com a eficiência; por exemplo, o GPT-4 alcançou 98.2% de Pass@10 no HumanEval, mas seu *runtime* médio foi 8.61, enquanto o DeepSeek Coder 33B Instruct, com 93.9% de Pass@10, teve um *runtime* mais eficiente de 7.54. Em problemas de maior complexidade, como no LeetCodeEval Médio, o GPT-4 demonstrou a maior eficiência média, com *runtime* de 50.92 e 73.09% Beats. Quanto à RQ2, sobre a otimização via prompts, as estratégias de "cadeia de pensamento" (gerar código e depois otimizar ou analisar a estratégia de otimização) se mostraram mais eficazes para problemas de média complexidade no LeetCodeEval, resultando em taxas de aceleração (*speedup*) notáveis. Por exemplo, o GPT-4 com o Prompt 3 alcançou uma aceleração de 1.18 no LeetCodeEval Médio, indicando que a análise passo a passo leva a melhorias mais significativas. O estudo conclui que a estratégia de treinamento e a natureza dos conjuntos de dados influenciam a eficiência, sugerindo que a otimização de prompts pode ser uma via promissora para melhorar o desempenho do código gerado.

## 2. Fichamento Bibliográfico

* *Large language models* (*LLMs*) (Modelos de Linguagem de Grande Escala) geram código a partir de descrições em linguagem natural, medido em *Pass@k* para correção funcional (página 103).
* HumanEval é um conjunto de dados de referência contendo 164 problemas de programação em Python usado para avaliar a correção funcional de código sintetizado a partir de docstrings, focando em compreensão de linguagem, algoritmos e matemática básica (página 104).
* *Runtime* (Tempo de Execução) é avaliado em *gem5 CPU simulator* para HumanEval e MBPP e por métricas de porcentagem de usuários superados em LeetCode, repetido múltiplas vezes para reduzir ruído (páginas 104–105).
* Pass@10 (Probabilidade de ao menos uma das 10 amostras passar) quantifica a correção funcional dos LLMs em *benchmarks*de síntese de programas (página 106).
* *Prompting* (Formulação de Perguntas) em três estilos—direto, otimização simples, e otimização guiada por análise de estratégia—melhora eficiência de código, especialmente em problemas de complexidade média no LeetCode (páginas 105–106).
* Modelos avaliados incluem GPT-4, GPT-3.5, Phi-2, Code Llama (7B, 13B, 34B), WizardCoder (7B, 13B, 34B) e DeepSeek Coder (33B *base* e *instruct*) (página 104).

## 3. Fichamento de Citações

* _"Efficient code can lead to higher performance and execution efficiency of programs and software completed by LLM-assisted programming."_
* _"We take the longest of these runtimes ... as the final runtime of the LLM on the problem."_
* _"The ability to generate correct code is not positively correlated with the ability to generate efficient code."_
* _"Larger number of parameters does not promise higher performance ... models of varying sizes share similar performance due to their reliance on the same training data."_
* _"Step-by-step prompting could make LLM to generate more efficient code, especially on complex problems."_
* _"Different from existing work that evaluate the correctness of the code generated by LLMs, we propose to further evaluate its efficiency."_
* _"Recommending more efficient code not only enhances program/software performance but also increase the probability of code acceptance by developers, reducing the need for further optimization and boosting development productivity."_